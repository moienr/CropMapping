{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "IN_KAGGLE = 'KAGGLE_URL_BASE' in os.environ\n",
    "IN_KAGGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_KAGGLE:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    secret_value = user_secrets.get_secret(\"gittoken2\")\n",
    "    !git clone https://{secret_value}@github.com/moienr/CropMapping.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_KAGGLE:\n",
    "    import time\n",
    "    import os\n",
    "    sleep_time = 5\n",
    "    while not os.path.exists(\"/kaggle/working/CropMapping\"):\n",
    "        print(\"didn't find the path, wating {sleep_time} more seconds...\")\n",
    "        time.sleep(sleep_time)\n",
    "    print(\"path found...\")\n",
    "    import sys\n",
    "    sys.path.append(\"/kaggle/working/CropMapping\")\n",
    "    sys.path.append(\"/kaggle/working/CropMapping/dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from model import UNET\n",
    "# from utils import (\n",
    "#     load_checkpoint,\n",
    "#     save_checkpoint,\n",
    "#     check_accuracy,\n",
    "#     save_predictions_as_imgs,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from glob import glob\n",
    "from skimage import io\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from datetime import date, datetime\n",
    "import json\n",
    "import pprint\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model import DualUNet3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dual_unet_3d():\n",
    "    print(\"Testing DualUNet3D...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "    s1_img = torch.randn((3, 2, 6, 64, 64)).to(device)\n",
    "    s2_img = torch.randn((3, 9, 6, 64, 64)).to(device)\n",
    "    model = DualUNet3D(s1_in_channels=2, s2_in_channels=9).to(device)\n",
    "    preds = model(s1_img, s2_img)\n",
    "    print(f\"Shape of s1_img: {s1_img.shape}\")\n",
    "    print(f\"Shape of s2_img: {s2_img.shape}\")\n",
    "    print(f\"Shape of preds: {preds.shape}\")\n",
    "    print(\"Success!\")\n",
    "\n",
    "test_dual_unet_3d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataset.data_loaders import *\n",
    "# from dataset.utils.utils import TextColors as TC\n",
    "# from dataset.utils.plot_utils import plot_s1s2_tensors, save_s1s2_tensors_plot\n",
    "# #from config import *\n",
    "# from train_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.data_loaders import *\n",
    "from plot import plot_train_test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_transform = transforms.Compose([NormalizeS1(),myToTensor(dtype=torch.float32)])\n",
    "s2_transform = transforms.Compose([NormalizeS2(),myToTensor(dtype=torch.float32)])\n",
    "crop_map_transform = transforms.Compose([CropMapTransform(),myToTensor(dtype=torch.float32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Paths\n",
    "if IN_KAGGLE:\n",
    "    s1_dir = \"/kaggle/input/france-valid-dataset-v1/s1/\"\n",
    "    s2_dir = \"/kaggle/input/france-valid-dataset-v1/s2/\"\n",
    "    crop_map_dir = \"/kaggle/input/france-valid-dataset-v1/crop_map/\"\n",
    "else:\n",
    "    s1_dir = \"D:\\\\python\\\\CropMapping\\\\dataset\\\\ts_dataset_patched\\\\s1\\\\\"\n",
    "    s2_dir = \"D:\\\\python\\\\CropMapping\\\\dataset\\\\ts_dataset_patched\\\\s2\\\\\"\n",
    "    crop_map_dir = \"D:\\\\python\\\\CropMapping\\\\dataset\\\\ts_dataset_patched\\\\crop_map\\\\\"\n",
    "    \n",
    "# Validation Paths\n",
    "if IN_KAGGLE:\n",
    "    s1_dir_valid = \"/kaggle/input/france-valid-dataset-v1/s1/\"\n",
    "    s2_dir_valid = \"/kaggle/input/france-valid-dataset-v1/s2/\"\n",
    "    crop_map_dir_valid = \"/kaggle/input/france-valid-dataset-v1/crop_map/\"\n",
    "else:\n",
    "    s1_dir_valid = \"D:\\\\python\\\\CropMapping\\\\dataset\\\\ts_dataset_patched\\\\s1\\\\\"\n",
    "    s2_dir_valid = \"D:\\\\python\\\\CropMapping\\\\dataset\\\\ts_dataset_patched\\\\s2\\\\\"\n",
    "    crop_map_dir_valid = \"D:\\\\python\\\\CropMapping\\\\dataset\\\\ts_dataset_patched\\\\crop_map\\\\\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Sen12Dataset(s1_dir=s1_dir,\n",
    "                            s2_dir=s2_dir,\n",
    "                            crop_map_dir=crop_map_dir,\n",
    "                            s1_transform=s1_transform,\n",
    "                            s2_transform=s2_transform,\n",
    "                            crop_map_transform=crop_map_transform,\n",
    "                            verbose=False)\n",
    "\n",
    "valid_dataset = Sen12Dataset(s1_dir=s1_dir_valid,\n",
    "                            s2_dir=s2_dir_valid,\n",
    "                            crop_map_dir=crop_map_dir_valid,\n",
    "                            s1_transform=s1_transform,\n",
    "                            s2_transform=s2_transform,\n",
    "                            crop_map_transform=crop_map_transform,\n",
    "                            verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[0][0].shape, train_dataset[0][1].shape, train_dataset[0][2].shape)\n",
    "print(valid_dataset[0][0].shape, valid_dataset[0][1].shape, valid_dataset[0][2].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DualUNet3D(s1_in_channels=2, s2_in_channels=10, out_channels=21,ts_depth=6,non_lin='sigmoid').to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, train_loader, criterion, optimizer, epoch, verbose=True):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch\n",
    "    \n",
    "    Args:\n",
    "    - model: the model to train\n",
    "    - train_loader: the data loader for the training data\n",
    "    - criterion: the loss function\n",
    "    - optimizer: the optimizer\n",
    "    - epoch: the current epoch\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Set the model to train mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    num_batches = len(train_loader)\n",
    "    # Loop over the data in the train loader\n",
    "    for batch_idx, (s1, s2, crop_map) in enumerate(train_loader):\n",
    "\n",
    "        # Move the data to the device\n",
    "        s1, s2, crop_map = s1.to(DEVICE), s2.to(DEVICE), crop_map.to(DEVICE)\n",
    "        # print(f\"s1.shape: {s1.shape}\", f\"s2.shape: {s2.shape}\", f\"crop_map.shape: {crop_map.shape}\")\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(s1, s2)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, crop_map)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        if verbose:\n",
    "            # Print the loss\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(s1)}/{len(train_loader.dataset)} '\n",
    "                f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "        \n",
    "    train_loss /= num_batches\n",
    "    return train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_step(model, valid_loader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set\n",
    "    \n",
    "    Args:\n",
    "    - model: the model to evaluate\n",
    "    - valid_loader: the data loader for the validation data\n",
    "    - criterion: the loss function\n",
    "    \n",
    "    Returns:\n",
    "    - val_loss: the average validation loss\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize the loss and number of samples\n",
    "    val_loss = 0.0\n",
    "    num_batches = len(valid_loader)\n",
    "\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "        # Loop over the data in the validation loader\n",
    "        for s1, s2, crop_map in valid_loader:\n",
    "            # Move the data to the device\n",
    "            s1, s2, crop_map = s1.to(DEVICE), s2.to(DEVICE), crop_map.to(DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(s1, s2)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, crop_map)\n",
    "\n",
    "            # Update the loss and number of samples\n",
    "            val_loss += loss.item() \n",
    "\n",
    "\n",
    "    # Calculate the average validation loss\n",
    "    val_loss /= num_batches\n",
    "\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_step(model, train_loader, criterion, optimizer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader,valid_loader, criterion, optimizer, num_epochs):\n",
    "    \"\"\"\n",
    "    Train the model for the specified number of epochs\n",
    "    \n",
    "    Args:\n",
    "    - model: the model to train\n",
    "    - train_loader: the data loader for the training data\n",
    "    - criterion: the loss function\n",
    "    - optimizer: the optimizer\n",
    "    - num_epochs: the number of epochs to train for\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    results = {\n",
    "    \"train_loss_history\": [],\n",
    "    \"val_loss_history\": []\n",
    "    }\n",
    "    progrss_bar = tqdm(range(num_epochs), desc=\"Training\", unit=\"epoch\")\n",
    "    for epoch in progrss_bar:\n",
    "        train_loss = train_step(model, train_loader, criterion, optimizer, epoch+1, verbose=False)\n",
    "        vali_loss = valid_step(model, valid_loader, criterion)\n",
    "        results[\"train_loss_history\"].append(train_loss)\n",
    "        results[\"val_loss_history\"].append(vali_loss)\n",
    "        \n",
    "        progrss_bar.set_postfix({\"Epoch\": epoch+1, \"Train Loss\": train_loss, \"Validation Loss\": vali_loss})\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results= train(model, train_loader,valid_loader, criterion, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to array with shape (1, len(list))\n",
    "train_loss_history = np.array(results[\"train_loss_history\"]).reshape(1, -1)\n",
    "val_loss_history = np.array(results[\"val_loss_history\"]).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_test_losses(train_loss_history, val_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(valid_loader))\n",
    "s1_img = batch[0].to(DEVICE)\n",
    "s2_img = batch[1].to(DEVICE)    \n",
    "crop_map = batch[2].to(DEVICE)\n",
    "print(f\"s1_img.shape: {s1_img.shape}\", f\"s2_img.shape: {s2_img.shape}\", f\"crop_map.shape: {crop_map.shape}\")\n",
    "output = model(s1_img, s2_img)\n",
    "print(f\"output.shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_s2_img(s2_img):\n",
    "    \"\"\"\n",
    "    Plot an image for each depth of the s2_img tensor, by plotting the first 3 channels as an RGB image.\n",
    "    \n",
    "    Args:\n",
    "    - s2_img: a tensor of shape (D, C, H, W)\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Move the tensor to the CPU and detach it\n",
    "    s2_img = s2_img.cpu().detach()\n",
    "    \n",
    "    # Permute the tensor to have shape (D, H, W, C)\n",
    "    s2_img = s2_img.permute(0, 2, 3, 1)\n",
    "    \n",
    "    # Loop over the depths and plot an image for each depth\n",
    "    for d in range(s2_img.shape[0]):\n",
    "        # Extract the first 3 channels as an RGB image\n",
    "        rgb_img = s2_img[d, :, :, :3]\n",
    "        \n",
    "        # Plot the RGB image\n",
    "        plt.imshow(rgb_img)\n",
    "        plt.title(f\"Depth {d}\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_s2_img(s2_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(output, dim=1) # if all values are 1, then the softmax is working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output[0].cpu().detach().numpy()\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_map = crop_map[0].cpu().detach().numpy()\n",
    "crop_map.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_output_crop_map(output, crop_map):\n",
    "    \"\"\"\n",
    "    Plot the model output and crop map side by side for each band\n",
    "    \n",
    "    Args:\n",
    "    - output: the model output tensor of shape (21, 64, 64)\n",
    "    - crop_map: the crop map tensor of shape (21, 64, 64)\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Loop over the bands\n",
    "    for i in range(output.shape[0]):\n",
    "        # Create a figure with two subplots\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        # Plot the model output in the first subplot\n",
    "        axs[0].imshow(output[i], cmap='gray')\n",
    "        axs[0].set_title(f'Band {i+1} - Model Output')\n",
    "\n",
    "        # Plot the crop map in the second subplot\n",
    "        axs[1].imshow(crop_map[i], cmap='gray')\n",
    "        axs[1].set_title(f'Band {i+1} - Crop Map')\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output_crop_map(output, crop_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
